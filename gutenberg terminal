Last login: Wed Nov 30 09:25:58 on ttys004
MacBooks-MacBook-8:~ macbook$ cd dsi-bigdata-vm
-bash: cd: dsi-bigdata-vm: No such file or directory
MacBooks-MacBook-8:~ macbook$ cd Documents
MacBooks-MacBook-8:Documents macbook$ ls
About Stacks.pdf
My Tableau Repository
Outcomes_Student_Action_Plan_What_Is_My_Brand___1_ (1).pdf
Outcomes_Student_Action_Plan_What_Is_My_Brand___1_.pdf
PocklingtonSaraOutcomesU1.pdf
Pocklington_Sara_OU4.pdf
Pocklington_Sara_Outcomes4.pdf
dsi-bigdata-vm
MacBooks-MacBook-8:Documents macbook$ cd dsi-bigdata-vm
MacBooks-MacBook-8:dsi-bigdata-vm macbook$ ls
Icon?
MacBooks-MacBook-8:dsi-bigdata-vm macbook$ ls
Icon?				dsi-bigdata-vm.box.crdownload
Vagrantfile
MacBooks-MacBook-8:dsi-bigdata-vm macbook$ vagrant up
Bringing machine 'default' up with 'virtualbox' provider...
==> default: Machine already provisioned. Run `vagrant provision` or use the `--provision`
==> default: flag to force provisioning. Provisioners marked to run always will still run.
MacBooks-MacBook-8:dsi-bigdata-vm macbook$ ls
Icon?				Vagrantfile			dsi-bigdata-vm.box.crdownload
MacBooks-MacBook-8:dsi-bigdata-vm macbook$ rm dsi-bigdata-vm.box.crdownload 
MacBooks-MacBook-8:dsi-bigdata-vm macbook$ ls
Icon?		Vagrantfile
MacBooks-MacBook-8:dsi-bigdata-vm macbook$ cp ~/Downloads/dsi-bigdata-vm.box .
^C
MacBooks-MacBook-8:dsi-bigdata-vm macbook$ 
MacBooks-MacBook-8:dsi-bigdata-vm macbook$ la
-bash: la: command not found
MacBooks-MacBook-8:dsi-bigdata-vm macbook$ ls
Icon?			Vagrantfile		dsi-bigdata-vm.box
MacBooks-MacBook-8:dsi-bigdata-vm macbook$ rm dsi-bigdata-vm.box 
MacBooks-MacBook-8:dsi-bigdata-vm macbook$ ls
Icon?		Vagrantfile
MacBooks-MacBook-8:dsi-bigdata-vm macbook$ cp ~/Downloads/dsi-bigdata-vm.vbox
usage: cp [-R [-H | -L | -P]] [-fi | -n] [-apvX] source_file target_file
       cp [-R [-H | -L | -P]] [-fi | -n] [-apvX] source_file ... target_directory
MacBooks-MacBook-8:dsi-bigdata-vm macbook$ cp ~/Downloads/dsi-bigdata-vm.vbox .
MacBooks-MacBook-8:dsi-bigdata-vm macbook$ ls
Icon?			Vagrantfile		dsi-bigdata-vm.vbox
MacBooks-MacBook-8:dsi-bigdata-vm macbook$ vagrant halt
==> default: Attempting graceful shutdown of VM...
    default: Guest communication could not be established! This is usually because
    default: SSH is not running, the authentication information was changed,
    default: or some other networking issue. Vagrant will force halt, if
    default: capable.
==> default: Forcing shutdown of VM...
MacBooks-MacBook-8:dsi-bigdata-vm macbook$ vagrant destroy
    default: Are you sure you want to destroy the 'default' VM? [y/N] y
==> default: Destroying VM and associated drives...
MacBooks-MacBook-8:dsi-bigdata-vm macbook$ vagrant up
Bringing machine 'default' up with 'virtualbox' provider...
==> default: Importing base box './dsi-bigdata-vm.box'...
==> default: Matching MAC address for NAT networking...
==> default: Setting the name of the VM: dsi-bigdata-vm
==> default: Clearing any previously set network interfaces...
==> default: Preparing network interfaces based on configuration...
    default: Adapter 1: nat
    default: Adapter 2: hostonly
==> default: Forwarding ports...
    default: 22 (guest) => 2222 (host) (adapter 1)
==> default: Running 'pre-boot' VM customizations...
==> default: Booting VM...
==> default: Waiting for machine to boot. This may take a few minutes...
    default: SSH address: 127.0.0.1:2222
    default: SSH username: vagrant
    default: SSH auth method: private key
    default: Warning: Remote connection disconnect. Retrying...
==> default: Machine booted and ready!
==> default: Checking for guest additions in VM...
    default: The guest additions on this VM do not match the installed version of
    default: VirtualBox! In most cases this is fine, but in rare cases it can
    default: prevent things such as shared folders from working properly. If you see
    default: shared folder errors, please make sure the guest additions within the
    default: virtual machine match the version of VirtualBox you have installed on
    default: your host and reload your VM.
    default: 
    default: Guest Additions Version: 4.3.36
    default: VirtualBox Version: 5.1
==> default: Configuring and enabling network interfaces...
MacBooks-MacBook-8:dsi-bigdata-vm macbook$ bigdata_start.sh
-bash: bigdata_start.sh: command not found
MacBooks-MacBook-8:dsi-bigdata-vm macbook$ vagrant ssh
Welcome to Ubuntu 14.04.4 LTS (GNU/Linux 3.13.0-87-generic x86_64)

 * Documentation:  https://help.ubuntu.com/

 System information disabled due to load higher than 1.0

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

New release '16.04.1 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


vagrant@vagrant-ubuntu-trusty-64:~$ bigdata_start.sh
Starting Hadoop DFS and Yarn
Starting namenodes on [0.0.0.0]
0.0.0.0: Warning: Permanently added '0.0.0.0' (ECDSA) to the list of known hosts.
0.0.0.0: starting namenode, logging to /usr/local/lib/hadoop-2.7.2/logs/hadoop-vagrant-namenode-vagrant-ubuntu-trusty-64.out
127.0.0.1: Warning: Permanently added '127.0.0.1' (ECDSA) to the list of known hosts.
127.0.0.1: starting datanode, logging to /usr/local/lib/hadoop-2.7.2/logs/hadoop-vagrant-datanode-vagrant-ubuntu-trusty-64.out
Starting secondary namenodes [0.0.0.0]
0.0.0.0: starting secondarynamenode, logging to /usr/local/lib/hadoop-2.7.2/logs/hadoop-vagrant-secondarynamenode-vagrant-ubuntu-trusty-64.out
Creating essential HDFS directories...
Starting.. YARN
starting yarn daemons
starting resourcemanager, logging to /usr/local/lib/hadoop-2.7.2/logs/yarn-vagrant-resourcemanager-vagrant-ubuntu-trusty-64.out
127.0.0.1: starting nodemanager, logging to /usr/local/lib/hadoop-2.7.2/logs/yarn-vagrant-nodemanager-vagrant-ubuntu-trusty-64.out
Starting Job History Server
nohup: redirecting stderr to stdout
Starting Hive... Metastore
Starting Hive...
Starting Spark...
starting org.apache.spark.deploy.history.HistoryServer, logging to /usr/local/lib/spark-1.6.1-bin-hadoop2.6/logs/spark-vagrant-org.apache.spark.deploy.history.HistoryServer-1-vagrant-ubuntu-trusty-64.out
starting org.apache.spark.deploy.master.Master, logging to /usr/local/lib/spark-1.6.1-bin-hadoop2.6/logs/spark-vagrant-org.apache.spark.deploy.master.Master-1-vagrant-ubuntu-trusty-64.out
starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/lib/spark-1.6.1-bin-hadoop2.6/logs/spark-vagrant-org.apache.spark.deploy.worker.Worker-1-vagrant-ubuntu-trusty-64.out
Starting Jupyter notebook
nohup: redirecting stderr to stdout
vagrant@vagrant-ubuntu-trusty-64:~$ hadoop fs -ls
vagrant@vagrant-ubuntu-trusty-64:~$ hadoop fs -ls/
-ls/: Unknown command
vagrant@vagrant-ubuntu-trusty-64:~$ hadoop fs -ls /
Found 3 items
drwxrwxrwx   - vagrant supergroup          0 2016-11-30 15:17 /tmp
drwxr-xr-x   - vagrant supergroup          0 2016-11-30 15:15 /user
drwxrwxrwx   - vagrant supergroup          0 2016-11-30 15:15 /var
vagrant@vagrant-ubuntu-trusty-64:~$ hadoop fs -mkdir wordcount-input
vagrant@vagrant-ubuntu-trusty-64:~$ echo "hello dear world hello" | hadoop fs -put - wordcount-input/hello.txt
vagrant@vagrant-ubuntu-trusty-64:~$ hadoop jar /usr/local/lib/hadoop-2.7.2/share/hadoop/mapreduce/hadoop*example*.jar wordcount wordcount-input wordcount-output
16/11/30 15:23:47 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
16/11/30 15:23:48 INFO input.FileInputFormat: Total input paths to process : 1
16/11/30 15:23:48 INFO mapreduce.JobSubmitter: number of splits:1
16/11/30 15:23:48 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1480518934907_0001
16/11/30 15:23:49 INFO impl.YarnClientImpl: Submitted application application_1480518934907_0001
16/11/30 15:23:49 INFO mapreduce.Job: The url to track the job: http://vagrant-ubuntu-trusty-64:8088/proxy/application_1480518934907_0001/
16/11/30 15:23:49 INFO mapreduce.Job: Running job: job_1480518934907_0001
16/11/30 15:24:01 INFO mapreduce.Job: Job job_1480518934907_0001 running in uber mode : false
16/11/30 15:24:01 INFO mapreduce.Job:  map 0% reduce 0%
16/11/30 15:24:08 INFO mapreduce.Job:  map 100% reduce 0%
16/11/30 15:24:16 INFO mapreduce.Job:  map 100% reduce 100%
16/11/30 15:24:16 INFO mapreduce.Job: Job job_1480518934907_0001 completed successfully
16/11/30 15:24:16 INFO mapreduce.Job: Counters: 49
	File System Counters
		FILE: Number of bytes read=41
		FILE: Number of bytes written=235127
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=141
		HDFS: Number of bytes written=23
		HDFS: Number of read operations=6
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Launched reduce tasks=1
		Data-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=5165
		Total time spent by all reduces in occupied slots (ms)=5225
		Total time spent by all map tasks (ms)=5165
		Total time spent by all reduce tasks (ms)=5225
		Total vcore-milliseconds taken by all map tasks=5165
		Total vcore-milliseconds taken by all reduce tasks=5225
		Total megabyte-milliseconds taken by all map tasks=5288960
		Total megabyte-milliseconds taken by all reduce tasks=5350400
	Map-Reduce Framework
		Map input records=1
		Map output records=4
		Map output bytes=39
		Map output materialized bytes=41
		Input split bytes=118
		Combine input records=4
		Combine output records=3
		Reduce input groups=3
		Reduce shuffle bytes=41
		Reduce input records=3
		Reduce output records=3
		Spilled Records=6
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=95
		CPU time spent (ms)=1310
		Physical memory (bytes) snapshot=333934592
		Virtual memory (bytes) snapshot=1325768704
		Total committed heap usage (bytes)=222101504
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=23
	File Output Format Counters 
		Bytes Written=23
vagrant@vagrant-ubuntu-trusty-64:~$ hadoop fs -cat wordcount-output/part*
dear	1
hello	2
world	1
vagrant@vagrant-ubuntu-trusty-64:~$ hadoop fs -copyFromLocal data/project_gutenberg project_gutenberg
vagrant@vagrant-ubuntu-trusty-64:~$ hadoop fs -copyFromLocal scripts scripts
vagrant@vagrant-ubuntu-trusty-64:~$ cat data/project_gutenberg/pg84.txt | python scripts/mapper.py | sort -k1,1 | python scripts/reducer.py
-	7
----	1




This Web site includes information about Project Gutenberg-tm,
including how to make donations to the Project Gutenberg Literary
Archive Foundation, how to help produce our new eBooks, and how to
subscribe to our email newsletter to hear about new eBooks.
Time taken: 0.909 seconds, Fetched: 3735 row(s)
hive> create external table alice as SELECT word, COUNT(*) FROM alice.text LATERAL VIEW explode(split(text, ' ')) lTable as word GROUP BY word;
FAILED: SemanticException [Error 10070]: CREATE-TABLE-AS-SELECT cannot create external table
hive> create table alice as SELECT word, COUNT(*) FROM alice.text LATERAL VIEW explode(split(text, ' ')) lTable as word GROUP BY word;
FAILED: SemanticException [Error 10001]: Line 1:49 Table not found 'text'
hive> create table alice as SELECT word, COUNT(*) FROM alice_text LATERAL VIEW explode(split(text, ' ')) lTable as word GROUP BY word;
WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. tez, spark) or using Hive 1.X releases.
Query ID = vagrant_20161130201502_f83e7bc8-0810-42be-baac-4aa3cda97c5e
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1480518934907_0004, Tracking URL = http://vagrant-ubuntu-trusty-64:8088/proxy/application_1480518934907_0004/
Kill Command = /usr/local/lib/hadoop-2.7.2/bin/hadoop job  -kill job_1480518934907_0004
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2016-11-30 20:38:21,132 Stage-1 map = 0%,  reduce = 0%
2016-11-30 20:38:31,219 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.41 sec
2016-11-30 20:38:41,014 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.51 sec
MapReduce Total cumulative CPU time: 4 seconds 510 msec
Ended Job = job_1480518934907_0004
Moving data to: hdfs://0.0.0.0/user/hive/warehouse/alice
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 4.51 sec   HDFS Read: 177414 HDFS Write: 58641 SUCCESS
Total MapReduce CPU Time Spent: 4 seconds 510 msec
OK
Time taken: 33.171 seconds
hive> 